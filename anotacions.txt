d3: main idea, implement a clone of jetpack joyride and add an AI to play it. To simplify it, there is only one state, the game state, no menus, no pause, no "you died" screen, etc.
d4: Pysics and logic, very simple physics. Gravity adds up, which makes the player go down, and force is applied when the player activates the input, moving the player up. Explain how the more gravity the harder to go up, etc
d5: Illusion of movement, to create the illusion of an infinite runner without using infinite resources, we use some camera tricks. The player never moves in the x axis. The ground and obstacles are the ones moving. To create this illusion, we add a ground, a background and some decorations and we move them using a technique called parallax scrolling. To achieve this effect, we use 2 or more layers (2 in our case) and we move them a little bit every frame to create the movement. To achieve the depth needed, the layers move at different speeds.
d6: sprites, in order to create our characters, we used a standard body with its animations along with the faces of our friends, to create custom characters.
d7: The obstacles are electric balls connected by lasers, which have 5 possible angles. The player must dodge these in order to survive. These are randomly generated every game, so that while training the AI we avoid overfitting. We will talk more about this later.
d8: Collision, the easier and most efficient (with a linear cost) to detect collision is called AABB collision, short for Axis Aligned Bounding Box. For this, the objects are surrounded with a rectangle that is aligned to the axis, and simple logic tests if one of the edges of the rectangle is inside the other. This is very efficient, but very inaccurate as we can see in the example.
d9: Mask collision, on the other hand, is very inefficient, but exactly accurate. It takes a mask (which is just an array of the size of the rectangle coding where there are pixels and where are not) and an offset (to align the rectangles) and it checks if any pixels are overlapping. To compensate for this inefficiency, we constantly check for AABB collisions (virtually no cost) and, once we detect a rectangle collision, we confirm its a real collision by using a mask collision.
d10: To implement the AI that will play our game, we will use the NEAT algorithm, short for Neuro Evolution of Augmented Topologies. The algorithm combines Neural Networks and Genetic algorithms to solve the problem. We implemented the algorithm by means of the neat-python library.
d11: Neural networks are a type of deep learning and AI that consist in using a group of nodes (known as neurons) and connecting them by means of a weighted directed graph, so that we have input neurons (at least one), hidden neurons (not mandatory) and output neurons (at least one), to mimic the behaviour of the human brain. How do they work? Input is given through the input neurons (human senses), is processed by the network, and an output is given (human reaction). Depending on the weight of the connections, the response to the same input may be different. The objective of a neural network is to perfect to weights of the connecions in order to give better responses to inputs. This is done by "training" the neural network with inputs we know the answer of, testing if the neural network is correct.
d12: The networks in our implementation have these inputs: ... and only one output: activating the jetpack or not. The activation function is tanh, which compresses the value of the response between -1 and 1.
d13: Genetic algorithms are the other part of the NEAT algorithm. They consist in creating a population, trying it, and continuing it only with the strongest individuals, inspired by the survival of the fittest by charles darwin. The first part of this algorithm is the selection. how do we decide which individual is the best? by using a fitness function. The higher the value of the function the best the player. In our game, an individual gains fitness every frame it survives, and also gains a bonus when surpassing a laser. Genetic operators are used to create newer individuals from the fittest of a generation, creating the next one. The operations are mutation (in which an individual modifies an attribute of itself randomly) and crossover/reproduction (in which two individuals combine their configurations to create a new one). Our configuration also implements elitism, so that for every generation, the 2 best individuals are carried on to the next unmodified, to guarantee at least a score as good as the last.
d14: To implement NEAT we took these 3 steps: First, adapting the game so there is not just one player, but multiple. Then, implementing the algorithm itself, defining the fitness function, the initial networks, the configs...Once done, we trained it and tested it with multiple configurations and evaluated the results we found.
d15: To start, we test out our algorithm with a population of 20 individuals per generation. Due to the small population size, improval is very rare. It has taken more than 20 generations to start getting past more than 150 fitness, which has been done purely out of luck. Still by generation 30 almost no visible improval is seen. By generation 36, the total record is 172 fitness. After the 36 generations, this is the neural network of the best fitness. As we can see, the output is calculated solely with the player height, which is probably the cause of its bad score. 
d16: A vast improvement is seen just by increasing the population size to 100, which reaches 600 fitness in just gen 2. This happens because the amount of population generates a lot more random possibilities, and thus getting us closer to a good AI in less time. On generation 12, the players have realized how to avoid obstacles and that they get bonus fitness for staying off the ground and away from the ceiling, reaching 1200 fitness. In this network we see that it uses both next vertical distances to the coils, which is a better signal than solely the player's height.
d17: By doubling the population, as expected, we get the same results in half the time. We see most inputs used in this network.
d18: We can try to make our AI react in advance of the next 2 lasers instead of only one by adding 4 inputs... If we use 9 inputs, the learning process is slower, but the skill of the resulting AI is greater. However, with just 20 members, we face the same problems as before, evolution is slow and high scores are purely based on luck. In this case, we see a clear example of overfitting. Since, as we said before, the obstacles are generated randomly, there is a chance that there are a large amount of obstacles that do not require the player to move away from the ceiling in order to be dodged. Then, a species may get a really high score by staying on the ceiling, which will create a network like this one, and then when a laser appears on the ceiling nothing is done to dodge it. As we can see, the network has no connections from the input to the output. This is because the AI detects that it can get a good score by just holding the jetpack. No real improvement except staying on the ceiling has been made by gen 24.
d19: With 9 inputs and a population of 100, it takes approximately 15 generations to realize the importance of not staying always in the ceiling, finally obtaining players actively dodging obstacles. However, it also takes some luck to get past 1000 by gen 10, because of the possible overfitting caused by obstacles not appearing on the ceiling or ground, same case as before. We can see more connections are used in this network.
d20: With 200 players per gen, we can see a player reach a fitness of 900 by gen 9. The large amount of players makes learning faster and more resistant to overfitting. A few generations later, at gen 21, it has reached 100000. We can see that the network uses the inputs that may be logic to us. It is a simple network that works very effectively. We will see that now.
d21: (aixo u diu el gala) Here we see some videos of the different stages of the game and AI, it is a little laggy and i thought it was a bad idea to put 3 videos on a single slide but my colleague here is very stubborn. Now, we can see the AI train in action (fiquem la IA)
d22: Conclusions: Improvisem
